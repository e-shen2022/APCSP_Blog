<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Title | Emma’s Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Title" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to a documentation of my emmazing life !!!!" />
<meta property="og:description" content="Welcome to a documentation of my emmazing life !!!!" />
<link rel="canonical" href="https://e-shen2022.github.io/emma_blog/2023/05/14/N@M.html" />
<meta property="og:url" content="https://e-shen2022.github.io/emma_blog/2023/05/14/N@M.html" />
<meta property="og:site_name" content="Emma’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-05-14T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Title" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-05-14T00:00:00-05:00","datePublished":"2023-05-14T00:00:00-05:00","description":"Welcome to a documentation of my emmazing life !!!!","headline":"Title","mainEntityOfPage":{"@type":"WebPage","@id":"https://e-shen2022.github.io/emma_blog/2023/05/14/N@M.html"},"url":"https://e-shen2022.github.io/emma_blog/2023/05/14/N@M.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/emma_blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://e-shen2022.github.io/emma_blog/feed.xml" title="Emma's Blog" /><link rel="shortcut icon" type="image/x-icon" href="/emma_blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/emma_blog/">Emma&#39;s Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/emma_blog/about/">About Me</a><a class="page-link" href="/emma_blog/vocab/">Vocab</a><a class="page-link" href="/emma_blog/categories/">Tags</a><a class="page-link" href="/emma_blog/search/">Search</a><a class="page-link" href="/emma_blog/JavaScript/">JavaScript</a><a class="page-link" href="/emma_blog/Frontend/">Frontend</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Title</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2023-05-14T00:00:00-05:00" itemprop="datePublished">
        May 14, 2023
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      15 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/e-shen2022/emma_blog/tree/master/_notebooks/2023-05-14-N@M.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/emma_blog/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/e-shen2022/emma_blog/master?filepath=_notebooks%2F2023-05-14-N%40M.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/emma_blog/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/e-shen2022/emma_blog/blob/master/_notebooks/2023-05-14-N@M.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/emma_blog/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Fe-shen2022%2Femma_blog%2Fblob%2Fmaster%2F_notebooks%2F2023-05-14-N%40M.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/emma_blog/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2023-05-14-N@M.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><h1>Translating Realtime Human Facial Expressions to an Emoji through a Trained CNN Algorithm <h1>&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2>Project Overview</h2><ol>
<li>Project Purpose/Description</li>
<li>Tool/Environment Setup </li>
<li>Theory Exploration (ML, NN, CNNs)</li>
<li>More imports &amp; Data Proprocessing</li>
<li>Create Model</li>
<li>Compile Model</li>
<li>Create your emojis</li>
<li>Implement GUI </li>
<li>Testing &amp; Improving Accuracy </li>
<li>Debugging</li>
<li>Reflection</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#1</mark> Project Goals/Description</h2><p>The goal of this project is to create a model capable of detecting human emotion through a realtime web cam and match the expression with a corresponding emoji.</p>
<p>For that we use a dataset containing more than 28700 images that is already classified in one of these 7 categories: angry, disgust, fear, happy, neutral, sad, and surprise.</p>
<p>We are going to create a machine learning algorithm, specifically a Convolutional Neural Network (CNN), with the platform Tensorflow to train the model based on this data to recognize facial expressions and map those same emotions on an emoji.</p>
<blockquote><p>Integrating the model with the frontend should result in a functionality that looks like this!</p>
</blockquote>
<p><img src="https://i.imgur.com/qZwnblY.png" alt="" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#2</mark> Tool/Environment Setup</h2><h3>Some tools/topics covered</h3><ul>
<li><p>Language: Python</p>
</li>
<li><p>Deep Neural Networks (Tensorflow)</p>
</li>
<li><p>Python Packages (Keras)</p>
</li>
</ul>
<h3>1. VSCode Environment</h3><p>(a) Create a new folder in File Explorer and name it <em>Project Name</em> in your C Drive (Directly in your OS folder)</p>
<p>(b) Open the folder in VSCODE</p>
<p>(c) Create a new folder called "src" and two new files called "train.py" and "emoji.py".</p>
<p>(d) Now create 2 subfolders under "src" called "data" and "emojis".</p>
<p>(e) Navigate to <a href="https://www.kaggle.com/datasets/msambare/fer2013">this dataset</a> on Kaggle and download it. We will be using this dataset to train our model so look around and familiarize yourself with what this data is!</p>
<p>(f) Download and extract the data into the "data" folder. You should now be able to see two subset folders labeled "train" and "test" folders with many pictures under the "data" folder.</p>
<p>We will be filling in the emojis folder later. This is all you need to set up for now!</p>
<h3>2. Modules to Install</h3><ul>
<li><p><b>OpenCV</b>: Otherwie known as Open Source Computer Vision. A library that provides a set of tools/functions to process/analyze images and videos</p>
</li>
<li><p><b>Numpy</b>: Python library that allows us to use multi-dimensional rrays to store large datasets and use optimized mathematical functions for data analysis</p>
</li>
<li><p><b>Tensorflow</b>: A very useful tool for machine learning. Takes data, builds a model, trains it, and then lets us use the trained model to make predictions!</p>
</li>
<li><p><b>Keras</b>: A high-level neural networks API integrated into Tensorflow</p>
</li>
</ul>
<p>Run these commands in terminal to install. These packages will later be used when compiling and training the model.</p>
<blockquote><p>FOR WINDOWS</p>
</blockquote>

<pre><code>pip install opencv-python

pip install numpy==1.22

pip install tensorflow==2.12.0 

pip install keras==2.12.0</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#3</mark> Theory Exploration: Machine Learning &amp; Neural Networks </h2><h3>Machine Learning</h3><p><img src="https://i.imgur.com/w8bT2HJ.png" alt="" /></p>
<ul>
<li><p>The term <nark>machine learning&lt;/mark&gt; has become a buzz word used by all those interested or knowledgable about the tech world. But what really is it?&lt;/p&gt;
&lt;/li&gt;
<li><p>To put it simply, machine learning is like <b>teaching a computer to learn things by itself</b>. Just like how a child is able to recognize what a dog is after many experiences of seeing or playing with a dog, if we show a computer lots of pictures of animals and tell it which animal is which, the computer will learn to recognize those animals by itself when given new pictures.</p>
</li>
<li><p>Thus, machine learning is a way for computers to detect patterns and make predictions based on data rather than being explicitly programmed to do a certain task.</p>
</li>
&lt;/ul&gt;
<h3>Neural Networks</h3><p><img src="https://i.imgur.com/3bORFz5.png" alt="" /></p>
<ul>
<li><p>A <mark>Neural Network</mark> is a type of machine learning model. It has 3 main types of layers: input, hidden, and output. It is designed <b>to work like a human brain by processing information through layers of connected neurons</b>.</p>
<ul>
<li><p>Each neuron recieves input, processes it, and then sends an output to the next layer of neurons.</p>
</li>
<li><p>Each layer learns to identify increasingly complex features and patterns, building on the features learned by the previous layers. For example, in an image recognition task the 1st layer might learn to identify simple features such as edges/corners and in the next layer it might learn to identify more complex features such as curves or textures that are made up of these simple features.</p>
</li>
</ul>
</li>
<li><p>In the picture above is an example of a <mark>Deep Neural Network</mark>, which is just a neural network with more than 2 hidden layers. These hidden layers are where most of the computations are made to identify patterns in the data and make predictions. The more # of hidden layers, the more the neural network is able to learn and recognize more COMPLEX patterns in the input data.</p>
</li>
</ul>
<h3>Why are we using a Deep Convolutional Neural Network (CNN)? </h3><p><img src="https://i.imgur.com/3RO81Ua.png" alt="" /></p>
<ul>
<li>A <mark>Convolutional Neural Network</mark> is a type of neural network that is <b>IDEAL for image classification</b> because it is specifically designed to recognize patterns and features within images <ul>
<li>Usually after the convolutional layers, there are <b>pooling layers</b> that look at small areas of the image, and then take the max/avg value in that area. This reduces the number of pixels in the image while keeping the most important info about the features for pattern recognition! </li>
</ul>
</li>
</ul>

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#4</mark> More imports &amp; Data Preprocessing </h2><p>Navigate to the <b>train.py file</b></p>
<h3>1. Import Packages </h3><blockquote>
<pre><code>import numpy as np 
from tensorflow import keras                                    
from keras.models import Sequential, load_model                   
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D
from keras.optimizers import Adam
from keras.layers import MaxPooling2D
from keras.preprocessing.image import ImageDataGenerator</code></pre>
</blockquote>
<p>We will learn about what these functions do soon~</p>
<hr />
<p><b>Before we can even start making our model, we need to pre-process our data. We will be rescaling, applying filters, and resizing the images to be compatible for NN training</b></p>
<h3>2. Train data </h3><h1 id="Define-the-directories-where-training/testing-data-located">Define the directories where training/testing data located<a class="anchor-link" href="#Define-the-directories-where-training/testing-data-located"> </a></h1><p>&gt;
    train_dir = 'data/train'
    value_dir = 'data/test'</p>
<h1 id="Divides-image-pixel-values-by-255-to-scale-down-pixel-values-to-normalized-range-between-0-and-1-for-NN-training">Divides image pixel values by 255 to scale down pixel values to normalized range between 0 and 1 for NN training<a class="anchor-link" href="#Divides-image-pixel-values-by-255-to-scale-down-pixel-values-to-normalized-range-between-0-and-1-for-NN-training"> </a></h1>
<pre><code>train_datagen = ImageDataGenerator(rescale=1./255)
value_datagen = ImageDataGenerator(rescale=1./255)


</code></pre>
<h1 id="Loads-images-from-train_dir">Loads images from train_dir<a class="anchor-link" href="#Loads-images-from-train_dir"> </a></h1><blockquote><p>
    train_generator = train_datagen.flow_from_directory(
        train_dir,</p>
</blockquote>
<h1 id="Resize-images-to-48-x-48-pixels">Resize images to 48 x 48 pixels<a class="anchor-link" href="#Resize-images-to-48-x-48-pixels"> </a></h1><p>&gt;
        target_size = (48, 48),</p>
<h1 id="Number-of-images-processed-in-each-batch">Number of images processed in each batch<a class="anchor-link" href="#Number-of-images-processed-in-each-batch"> </a></h1><p>&gt;
        batch_size = 64,</p>
<h1 id="Convert-images-to-grayscale-(reduce-dimensionality-of-input-data-from-RGB-to-intensity)">Convert images to grayscale (reduce dimensionality of input data from RGB to intensity)<a class="anchor-link" href="#Convert-images-to-grayscale-(reduce-dimensionality-of-input-data-from-RGB-to-intensity)"> </a></h1><p>&gt;
        color_mode = "grayscale",</p>
<h1 id="Labels-for-images-are-categorical-values-(Ex.-Happy,-Sad,-Surprised,-etc)">Labels for images are categorical values (Ex. Happy, Sad, Surprised, etc)<a class="anchor-link" href="#Labels-for-images-are-categorical-values-(Ex.-Happy,-Sad,-Surprised,-etc)"> </a></h1><p>&gt;
        class_mode = 'categorical'
        )</p>
<h1 id="Same-process-for-test-data">Same process for test data<a class="anchor-link" href="#Same-process-for-test-data"> </a></h1><blockquote><p><br />
    value_generator = value_datagen.flow_from_directory(
        train_dir,
        target_size = (48, 48),
        batch_size = 64,
        color_mode = "grayscale",
        class_mode = 'categorical'
        )</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#5</mark> Create Model </h2><p>Continue in the train.py file.</p>
<p>We can now start building our Convolutional Neural Network layer by layer using the sequential model.</p>
<p>In order to create an <b>accurate</b> model, we are going to implement many convolutional layers to detect complex patterns, pooling layers to downsample the data, regularization techniques to prevent overfitting, flatten layers to prepare for fully connected layers, and dense layers to prepare for classification using activation functions.</p>
<p><hr /></p>
<blockquote><p>emotion_model = Sequential()</p>
</blockquote>
<h1 id="Adding-2-convolutional-layers-that-are-responsible-for-detecting-local-patterns-in-the-input-data">Adding 2 convolutional layers that are responsible for detecting local patterns in the input data<a class="anchor-link" href="#Adding-2-convolutional-layers-that-are-responsible-for-detecting-local-patterns-in-the-input-data"> </a></h1><h1 id="1st-layer-has-32-filters-of-size-3x3-pixels-and-applies-the-ReLU-activation-function,-2nd-is-the-same-except-has-64-filters-that-allows-the-model-to-extract-more-complex-patterns">1st layer has 32 filters of size 3x3 pixels and applies the ReLU activation function, 2nd is the same except has 64 filters that allows the model to extract more complex patterns<a class="anchor-link" href="#1st-layer-has-32-filters-of-size-3x3-pixels-and-applies-the-ReLU-activation-function,-2nd-is-the-same-except-has-64-filters-that-allows-the-model-to-extract-more-complex-patterns"> </a></h1><blockquote>
<pre><code>emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))
emotion_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))</code></pre>
</blockquote>
<h1 id="Pooling-layers:Downsample-data-to-look-at-small-areas-of-the-image-(reduces-spatial-dimensions-while-retaining-important-features)&gt;-emotion_model.add(MaxPooling2D(pool_size=(2,2)))">Pooling layers:Downsample data to look at small areas of the image (reduces spatial dimensions while retaining important features)&gt; emotion_model.add(MaxPooling2D(pool_size=(2,2)))<a class="anchor-link" href="#Pooling-layers:Downsample-data-to-look-at-small-areas-of-the-image-(reduces-spatial-dimensions-while-retaining-important-features)&gt;-emotion_model.add(MaxPooling2D(pool_size=(2,2)))"> </a></h1><h1 id="Regularization:Randomly-sets-input-units-to-0-to-prevent-overfitting-(learn-noise-rather-than-actual-signal)&gt;-emotion_model.add(Dropout(0.25))">Regularization:Randomly sets input units to 0 to prevent overfitting (learn noise rather than actual signal)&gt; emotion_model.add(Dropout(0.25))<a class="anchor-link" href="#Regularization:Randomly-sets-input-units-to-0-to-prevent-overfitting-(learn-noise-rather-than-actual-signal)&gt;-emotion_model.add(Dropout(0.25))"> </a></h1><h1 id="More-convolutional/pooling-layers-and-regularization">More convolutional/pooling layers and regularization<a class="anchor-link" href="#More-convolutional/pooling-layers-and-regularization"> </a></h1><p>&gt;
    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
    emotion_model.add(MaxPooling2D(pool_size=(2,2)))
    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))
    emotion_model.add(MaxPooling2D(pool_size=(2,2)))
    emotion_model.add(Dropout(0.25))</p>
<h1 id="Reshapes-ouput-from-previous-layers-into-1D-vector-to-prepare-for-the-fully-connected-layers">Reshapes ouput from previous layers into 1D vector to prepare for the fully connected layers<a class="anchor-link" href="#Reshapes-ouput-from-previous-layers-into-1D-vector-to-prepare-for-the-fully-connected-layers"> </a></h1><p>&gt;
    emotion_model.add(Flatten())</p>
<h1 id="1st-dense-layer:1024-neurons-fully-connected-layer&gt;-emotion_model.add(Dense(1024,-activation='relu'))">1st dense layer:1024 neurons fully connected layer&gt; emotion_model.add(Dense(1024, activation='relu'))<a class="anchor-link" href="#1st-dense-layer:1024-neurons-fully-connected-layer&gt;-emotion_model.add(Dense(1024,-activation='relu'))"> </a></h1>
<pre><code>emotion_model.add(Dropout(0.5))

</code></pre>
<h1 id="2nd-dense-layer:7-neurons-which-represents-the-#-of-possible-output-classes-(emotions)#Uses-softmax-activation-function-to-convert-final-layer's-raw-predicted-values-into-a-probability-distribution-over-the-different-classes-for-classification">2nd dense layer:7 neurons which represents the # of possible output classes (emotions)#Uses softmax activation function to convert final layer's raw predicted values into a probability distribution over the different classes for classification<a class="anchor-link" href="#2nd-dense-layer:7-neurons-which-represents-the-#-of-possible-output-classes-(emotions)#Uses-softmax-activation-function-to-convert-final-layer's-raw-predicted-values-into-a-probability-distribution-over-the-different-classes-for-classification"> </a></h1><p>&gt;
    emotion_model.add(Dense(7, activation='softmax'))</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#6</mark> Compile the Model </h2><h1 id="Prepare-model-for-training-by-defining-how-it-will-measure-loss,-update-its-weights,-and-evaluate-its-prediction-performance">Prepare model for training by defining how it will measure loss, update its weights, and evaluate its prediction performance<a class="anchor-link" href="#Prepare-model-for-training-by-defining-how-it-will-measure-loss,-update-its-weights,-and-evaluate-its-prediction-performance"> </a></h1><blockquote><p>emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])</p>
</blockquote>
<h1 id="Train-the-model-using-a-generator">Train the model using a generator<a class="anchor-link" href="#Train-the-model-using-a-generator"> </a></h1><p>&gt;
    emotion_model_info = emotion_model.fit_generator(
        train_generator,</p>

<pre><code>    #Number of batches processed in each epoch
    steps_per_epoch =28709 // 64,

    #Number of times the entire dataset is passed through the model for training
    epochs=50,

    validation_data=validation_generator,

    #Number of batches to be processed for validation in each epoch
    validation_steps=7178 // 64 
)

</code></pre>
<h1 id="Save-learned-parameters">Save learned parameters<a class="anchor-link" href="#Save-learned-parameters"> </a></h1><p>&gt;
    emotion_model.save_weights('model.h5')</p>
<hr />

<p><b>Congratulations! All the code is in place, and you are now ready to compile and train your model!</b></p>
<ul>
<li>Go into your terminal and cd into the src folder</li>
<li><p>Run the line below and watch your neural network slowly but surely go through the epoch 50 times! 
&gt;
  python train.py</p>
</li>
<li><p>The output should look something like this. Watch the loss decrease and the accuracy increase !</p>
</li>
</ul>
<p>Start:&gt;<img src="https://i.imgur.com/0WzdMVU.png" alt="" /></p>
<p>End:</p>
<blockquote><p><img src="https://i.imgur.com/SkgIxW5.png" alt="" /></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#7</mark> Fun! Create your personalized emojis :) </h2><p><b>Now is time to design our emojis that will match with our realtime human facial detection</b></p>
<ul>
<li>Follow this <a href="https://getavataaars.com/?accessoriesType=Blank&amp;avatarStyle=Transparent&amp;clotheColor=Red&amp;clotheType=BlazerSweater&amp;eyeType=Default&amp;hairColor=Black&amp;mouthType=Default&amp;topType=LongHairNotTooLong">link</a> and create 7 seperate emojis for each of the 7 emotions</li>
<li>Save the images as angry.png, disgusted.png, fearful.png, happy.png, neutral.png, sad.png, surprised.png and place them in the "emojis" folder</li>
</ul>
<p>Here are some examples for inspo:</p>
<p>Happy</p>
<p><img src="https://i.imgur.com/eLvInGM.png" alt="" /></p>
<p>Surprised</p>
<p><img src="https://i.imgur.com/Kxz21Ci.png" alt="" /></p>
<p>Disgusted</p>
<p><img src="https://i.imgur.com/nVGEWFB.png" alt="" /></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#8</mark> Implement GUI </h2><p>Now we will work in the <b>emoji.py</b></p>
<h3>1. Import Packages</h3>
&gt; import tkinter as tk 
from tkinter import * 
import cv2 
from PIL import Image, ImageTk
import os
from cv2 import CAP_V4L2
import numpy as np
import cv2 
from keras.models import Sequential 
from keras.layers import Dense, Dropout, Flatten 
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
import threading 
import time

<h3>2. Copy Code </h3><p>Copy the following from the train.py file
&gt;</p>

<pre><code>emotion_model = Sequential()

emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))
emotion_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))

emotion_model.add(MaxPooling2D(pool_size=(2,2)))

emotion_model.add(Dropout(0.25))

emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))
emotion_model.add(MaxPooling2D(pool_size=(2,2)))
emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))
emotion_model.add(MaxPooling2D(pool_size=(2,2)))
emotion_model.add(Dropout(0.25))

emotion_model.add(Flatten())

emotion_model.add(Dense(1024, activation='relu'))
emotion_model.add(Dropout(0.5))

emotion_model.add(Dense(7, activation='softmax'))
emotion_model.load_weights('model.h5')


</code></pre>
<h3>3. Create dictionaries to later access emotion text and emojis</h3><blockquote>
</blockquote>

<pre><code>#Disable use of OpenCV
cv2.ocl.setUseOpenCL(False)

#Create dictionary of emotions 
emotion_dict = {
    0:"   Angry   ",         1: "   Disgusted   ", 
    2: "   Fearful   ", 
    3: "   Happy   ", 
    4: "   Neutral   ", 
    5: "   Sad   ", 
    6: "   Surprised   "}

#Generate path
cur_path = os.path.dirname(os.path.abspath(__file__))

#Navigate from current path into emojis folder and pick corresponding emotion
emoji_dist = {
    0: cur_path+"/emojis/angry.png",
    1: cur_path+"/emojis/disgusted.png",
    2: cur_path+"/emojis/fearful.png",
    3: cur_path+"/emojis/happy.png",
    4: cur_path+"/emojis/neutral.png",
    5: cur_path+"/emojis/sad.png",
    6: cur_path+"/emojis/surprised.png",
}

</code></pre>
<h3> 4. Initialize variables and arrays </h3>
&gt; #Stores the last captured video frame
    global last_frame1
    #Initializes array of 0s that will pass in image's RGB values 
    last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)
    global cap1 
    #Initialize a list with a single element (index from emotion dictionary) 
    show_text = [0] 
    #Default emoji index (neutral)
    show_text[0] = 4
    #Event to synchronize subject and avatar threads
    switch_thread_event = threading.Event()
    #Event to signal the reads when to stop execution
    stop_event = threading.Event()

    # Debug counters
    subject_count = 0 
    avatar_count = 0

<h3> 5. Create function to capture and read subject in frame</h3><blockquote>
</blockquote>
<h1 id="Function-to-capture-video-frames-from-webcam-and-detect-emotions-on-the-subject's-face">Function to capture video frames from webcam and detect emotions on the subject's face<a class="anchor-link" href="#Function-to-capture-video-frames-from-webcam-and-detect-emotions-on-the-subject's-face"> </a></h1><p>def show_subject():global subject_count</p>

<pre><code>#While program still running
while not stop_event.is_set():
    # Wait for the switch thread event to be set
    if not switch_thread_event.wait(5):
        print("Subject Timeout occurred!")
        break

    #Open webcam
    cap1 = cv2.VideoCapture(0)

    if not cap1.isOpened():
        print("Can't find the camera")
    else:
        print("Opened Camera")

    # frame 1 captures the video frame by frame, flag1 returns frame status 
    flag1, frame1 = cap1.read()

    #Resize frame for faster processing
    frame1 = cv2.resize(frame1, (600,500))

    #Haarcascade classifier detects face in frame using pretrained info ab facial features
    bounding_box = cv2.CascadeClassifier('C:\Emojify\data\haarcascade_frontalface_default.xml')
    #Converted to grayscale for better face detection accuracy
    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)

    #Note: Adjust scaleFactor and minNeighbors for prediction accuracy 
    #detectMultiScale function returns the coordinates and dimensions of the detected faces as rectangles
    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor = 1.1, minNeighbors=7)

    #For each detected face, a rectangle is drawn around it on the frame
    for (x, y, w, h) in num_faces: 
        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)
        roi_gray_frame = gray_frame[y:y+h, x:x+w]
        #Region of interest resized to the expected input size for the emotion recognition model
        #Face image converted into a numpy array
        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)
        #Face image is passed through the pre-trained emotion recognition model 
        #Predict function returns a probability distribution over different emotion classes
        prediction = emotion_model.predict(cropped_img)
        #Index of the highest probability in the prediction array is calculated to determine the predicted emotion class
        maxindex = int(np.argmax(prediction))
        #Retrieve emotion label &amp; display (subject)
        #Corresponding emotion label is retrieved from the emotion_dict dictionary &amp; displayed in window
        cv2.putText(frame1, emotion_dict[maxindex], (x+40, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)

        show_text[0]=maxindex

        #For debugging
        current_time_ms = time.time_ns() // 10**6
        subject_count = subject_count + 1 
        print("Current time for subject", current_time_ms, show_text[0], subject_count)

    print("flag1", flag1)

    #If frame is not returned
    if flag1 is None:
        print("Major error! Frame is not returned!")
    #If frame captured successfully 
    elif flag1 == True:
        global last_frame1
        last_frame1 = frame1.copy()
        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(pic)
        #Represents image element in GUI
        imgtk = ImageTk.PhotoImage(image=img)
        #Updates displayed image
        lmain.imgtk = imgtk
        lmain.configure(image=imgtk)

    # After loop release webcam to be used in this program
    #cap1.release()
    # Destroy all the windows
    cv2.destroyAllWindows()
    #Never prints bc webcam not released 
    print('webcam destroyed')

    #Once process frame, pause for a bit
    time.sleep(0.3)
    #Update window
    root.update()

    # Reset the switch_thread_event
    switch_thread_event.clear()
    switch_thread_event.set()

print("Subject thread is finished")

</code></pre>
<p><h3> 6. Create avatar function to sink subject emotions with displayed emoji</h3></p>
<blockquote><p>def show_avatar():global avatar_count
    while not stop_event.is_set():</p>

<pre><code>    # Wait for the switch_thread_event to be set
    if not switch_thread_event.wait(5):
        print("Avatar Timeout occurred!")
        break</code></pre>
</blockquote>

<pre><code>    #More debugging
    emoji_index = show_text[0]
    avatar_count = avatar_count + 1 
    current_time_ms = time.time_ns() // 10**6
    print("Current time for avatar", current_time_ms, emoji_index, avatar_count)

    frame2 = cv2.imread(emoji_dist[emoji_index])
    pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)
    img2 = Image.fromarray(pic2)
    imgtk2 = ImageTk.PhotoImage(image=img2)
    lmain2.imgtk2 = imgtk2
    lmain3.configure(text = emotion_dict[emoji_index], font = ('arial', 45, 'bold'))
    lmain2.configure(image=imgtk2)

    time.sleep(0.3)
    root.update()

    # Reset the switch_thread_event
    switch_thread_event.clear()
    switch_thread_event.set()

print ("Avatar thread is finished")

</code></pre>
<p><h3> 7. Wrapper functions to call stop and switch tread functions</h3></p>
<blockquote><p>def stop_threads():global stop_event        global switch_thread_event</p>

<pre><code>    #Stop events, clear threads
    stop_event.set()
    switch_thread_event.clear()</code></pre>
</blockquote>

<pre><code>def wrapper_quit():
    stop_threads()
    print("After stop camera thread ", stop_event.is_set(), switch_thread_event.is_set())
    #Close GUI window
    root.destroy()

</code></pre>
<p><h3> 8. if main &amp; using tkInter to display frontend by placing and packing labels</h3></p>
<blockquote><h1 id="Only-be-executed-if-script-run-directly">Only be executed if script run directly<a class="anchor-link" href="#Only-be-executed-if-script-run-directly"> </a></h1><p>if <strong>name</strong> == '<strong>main</strong>':frame_number = 0    root = tk.Tk()</p>
</blockquote>

<pre><code>#Create labels to contain images/video
#Human video
lmain = tk.Label(master = root, padx = 50, bd = 10)
#Emoji
lmain2 = tk.Label(master = root, bd = 25)
#Quit button for entire program
lmain3 = tk.Label(master=root, bd = 20, fg = "#CDCDCD", bg = 'blue', font=("Arial", 30))

#Packing and placing in location 
lmain.pack(side=LEFT)
lmain.place(x = 30, y = 100)
lmain3.pack()
lmain3.place(x = 1000, y = 600)
lmain2.pack(side=RIGHT)
lmain2.place(x = 700, y = 100)

root.title("Translating Realtime Human Facial Expressions to an Emoji using a Trained CNN")
root.geometry("1400x900+100+10")
root['bg'] = 'black'
switch_thread_event.set() 
subject_thread = threading.Thread(target = show_subject)
avatar_thread = threading.Thread(target = show_avatar)
#When button pressed, function specified by command parameter will be executed
exitButton = Button(root, text = 'Quit', fg = "red", command = wrapper_quit, font = ('arial', 30, 'bold')).pack(side = TOP)

subject_thread.start()
avatar_thread.start()

print("Before main loop")
root.mainloop()</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2><mark>#9</mark> Testing &amp; Improving Accuracy</h2><p><b> Do a happy dance! You have now officially coded the base to start testing your final product!</b></p>
<ul>
<li>Run python3 emoji.py in terminal and watch magic happen!</li>
</ul>
<p>Some advise about <mark>improving accuracy</mark></p>
<ul>
<li>Subject: Wear plain clothes (avoid graphics), Hair out of face, One person in frame at a time, ONLY use face to change expression (face detection XML file doesn't take into account hands or any other body parts)</li>
<li>Adjust scalefactor and minNeighbors in emoji.py </li>
<li>Consider adding more diverse photos in data set and retraining the model </li>
</ul>
<p>NOTES about the model</p>
<ol>
<li>Accuracy of certain facial expressions </li>
</ol>
<ul>
<li>Happy (Curve of mouth, Eyes are smaller) - GOOD </li>
<li>Surprised (Form an O with mouth, big eyes) - GOOD </li>
<li>Neutral (Flat line of mouth, eyebrows even, eyes not big) - OKAY </li>
<li>Sad (Mouth curved down, smaller eyes) - OKAY </li>
<li>Anger (exaggerated frustrated eyebrows and mouth curved downword) - OKAY</li>
<li>Disgusted - BAD RARELY DETECTS </li>
<li>Fearful (Wide eyes, mouth open) - BAD, EASILY CONFUSED WITH SURPRISED</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Extra notes</p>
<ul>
<li>Dense layer: Each neuron in the layer is connected to EVERY neuron in the previous layer</li>
<li>Drop out Layer: Randomly deactivates neurons to prevent a model that becomes too specialized and preforms extremely well on the training data but fails to generalize and make accurate predictions on new, unseen data </li>
<li>Flatten layer: Takes complex, structured data (images) and makes its impler by converting it into a flat, 1D array. Useful when transitioning from convolutionl/pooling layers to subsequent layers to process the data as a simpler, linear sequence. Easier for NN to learn patterns and make predictions </li>
<li>Keras: Sequential vs Functional<ul>
<li>Sequential (Create model layer by layer)</li>
<li>Functional (A layer can connect to any layer, much more complex)</li>
</ul>
</li>
<li>CNN: Simple pplication of filter to an input that results in an activation. Certain inputs and thresholds. When Input meets those thresholds there is an activation </li>
<li>Certain type of input --&gt; repeats itself --&gt; feature map forms</li>
<li>Activation function: mathematical func that determines whether the neuron should be activated based on the input it recieves. Introduces non-linearity to the network, allowing it to learn and model complex relationships between input data and output predictions </li>
</ul>

</div>
</div>
</div>
&lt;/div&gt;
 

</nark></p></li></ul></div></div></div></h1></h1></p></div></div></div></div>


  </div><a class="u-url" href="/emma_blog/2023/05/14/N@M.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/emma_blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://e-shen2022.github.io/emma_blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/emma_blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Welcome to a documentation of my emmazing life !!!!</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li>
  <a rel="me" href="” https://github.com/nighthawkcoders “" target="_blank" title="github">
    <svg class="svg-icon grey">
      <use xlink:href="/emma_blog/assets/minima-social-icons.svg#github"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="” https://twitter.com/NighthawkCoding “" target="_blank" title="twitter">
    <svg class="svg-icon grey">
      <use xlink:href="/emma_blog/assets/minima-social-icons.svg#twitter"></use>
    </svg>
  </a>
</li>
<li>
  <a rel="me" href="” https://www.youtube.com/channel/UClIKOsDS5dsfzFA3zveDT3Q “" target="_blank" title="youtube">
    <svg class="svg-icon grey">
      <use xlink:href="/emma_blog/assets/minima-social-icons.svg#youtube"></use>
    </svg>
  </a>
</li>
</ul>
</div>

  </div>

</footer>
</body>

</html>
