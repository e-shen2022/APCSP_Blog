{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Translating Realtime Human Facial Expressions to an Emoji through a Trained CNN Algorithm <h1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project Overview</h2>\n",
    "\n",
    "1. Project Purpose/Description\n",
    "2. Tool/Environment Setup \n",
    "3. Theory Exploration (ML, NN, CNNs)\n",
    "4. More imports & Data Proprocessing\n",
    "5. Create Model\n",
    "6. Compile Model\n",
    "7. Create your emojis\n",
    "8. Implement GUI \n",
    "9. Testing & Improving Accuracy \n",
    "10. Debugging\n",
    "11. Reflection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#1</mark> Project Goals/Description</h2>\n",
    "\n",
    "The goal of this project is to create a model capable of detecting human emotion through a realtime web cam and match the expression with a corresponding emoji. \n",
    "\n",
    "For that we use a dataset containing more than 28700 images that is already classified in one of these 7 categories: angry, disgust, fear, happy, neutral, sad, and surprise. \n",
    "\n",
    "We are going to create a machine learning algorithm, specifically a Convolutional Neural Network (CNN), with the platform Tensorflow to train the model based on this data to recognize facial expressions and map those same emotions on an emoji. \n",
    "\n",
    "> Integrating the model with the frontend should result in a functionality that looks like this!\n",
    "\n",
    "![](https://i.imgur.com/qZwnblY.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#2</mark> Tool/Environment Setup</h2>\n",
    "\n",
    "<h3>Some tools/topics covered</h3>\n",
    "\n",
    "- Language: Python\n",
    "\n",
    "- Deep Neural Networks (Tensorflow)\n",
    "\n",
    "- Python Packages (Keras)\n",
    "\n",
    "<h3>1. VSCode Environment</h3>\n",
    "\n",
    "(a) Create a new folder in File Explorer and name it *Project Name* in your C Drive (Directly in your OS folder)\n",
    "\n",
    "(b) Open the folder in VSCODE \n",
    "\n",
    "(c) Create a new folder called \"src\" and two new files called \"train.py\" and \"emoji.py\". \n",
    "\n",
    "(d) Now create 2 subfolders under \"src\" called \"data\" and \"emojis\". \n",
    "\n",
    "(e) Navigate to [this dataset](https://www.kaggle.com/datasets/msambare/fer2013) on Kaggle and download it. We will be using this dataset to train our model so look around and familiarize yourself with what this data is!\n",
    "\n",
    "(f) Download and extract the data into the \"data\" folder. You should now be able to see two subset folders labeled \"train\" and \"test\" folders with many pictures under the \"data\" folder. \n",
    "\n",
    "We will be filling in the emojis folder later. This is all you need to set up for now!\n",
    "\n",
    "\n",
    "<h3>2. Modules to Install</h3>\n",
    "\n",
    "- <b>OpenCV</b>: Otherwie known as Open Source Computer Vision. A library that provides a set of tools/functions to process/analyze images and videos \n",
    "\n",
    "- <b>Numpy</b>: Python library that allows us to use multi-dimensional rrays to store large datasets and use optimized mathematical functions for data analysis\n",
    "\n",
    "- <b>Tensorflow</b>: A very useful tool for machine learning. Takes data, builds a model, trains it, and then lets us use the trained model to make predictions!\n",
    "\n",
    "- <b>Keras</b>: A high-level neural networks API integrated into Tensorflow\n",
    "\n",
    "Run these commands in terminal to install. These packages will later be used when compiling and training the model. \n",
    "\n",
    "> FOR WINDOWS \n",
    "\n",
    "    pip install opencv-python\n",
    "\n",
    "    pip install numpy==1.22\n",
    "\n",
    "    pip install tensorflow==2.12.0 \n",
    "    \n",
    "    pip install keras==2.12.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#3</mark> Theory Exploration: Machine Learning & Neural Networks </h2>\n",
    "\n",
    "<h3>Machine Learning</h3>\n",
    "\n",
    "![](https://i.imgur.com/w8bT2HJ.png)\n",
    "\n",
    "- The term <nark>machine learning</mark> has become a buzz word used by all those interested or knowledgable about the tech world. But what really is it? \n",
    "\n",
    "- To put it simply, machine learning is like <b>teaching a computer to learn things by itself</b>. Just like how a child is able to recognize what a dog is after many experiences of seeing or playing with a dog, if we show a computer lots of pictures of animals and tell it which animal is which, the computer will learn to recognize those animals by itself when given new pictures. \n",
    "\n",
    "- Thus, machine learning is a way for computers to detect patterns and make predictions based on data rather than being explicitly programmed to do a certain task. \n",
    "\n",
    "<h3>Neural Networks</h3>\n",
    "\n",
    "![](https://i.imgur.com/3bORFz5.png)\n",
    "\n",
    "- A <mark>Neural Network</mark> is a type of machine learning model. It has 3 main types of layers: input, hidden, and output. It is designed <b>to work like a human brain by processing information through layers of connected neurons</b>. \n",
    "\n",
    "    - Each neuron recieves input, processes it, and then sends an output to the next layer of neurons. \n",
    "\n",
    "    - Each layer learns to identify increasingly complex features and patterns, building on the features learned by the previous layers. For example, in an image recognition task the 1st layer might learn to identify simple features such as edges/corners and in the next layer it might learn to identify more complex features such as curves or textures that are made up of these simple features. \n",
    "\n",
    "- In the picture above is an example of a <mark>Deep Neural Network</mark>, which is just a neural network with more than 2 hidden layers. These hidden layers are where most of the computations are made to identify patterns in the data and make predictions. The more # of hidden layers, the more the neural network is able to learn and recognize more COMPLEX patterns in the input data. \n",
    "\n",
    "<h3>Why are we using a Deep Convolutional Neural Network (CNN)? </h3>\n",
    "\n",
    "![](https://i.imgur.com/3RO81Ua.png)\n",
    "\n",
    "- A <mark>Convolutional Neural Network</mark> is a type of neural network that is <b>IDEAL for image classification</b> because it is specifically designed to recognize patterns and features within images \n",
    "    - Usually after the convolutional layers, there are <b>pooling layers</b> that look at small areas of the image, and then take the max/avg value in that area. This reduces the number of pixels in the image while keeping the most important info about the features for pattern recognition! \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#4</mark> More imports & Data Preprocessing </h2>\n",
    "\n",
    "Navigate to the <b>train.py file</b>\n",
    "\n",
    "<h3>1. Import Packages </h3>\n",
    "\n",
    "> \n",
    "    import numpy as np \n",
    "    from tensorflow import keras                                    \n",
    "    from keras.models import Sequential, load_model                   \n",
    "    from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.layers import Conv2D\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers import MaxPooling2D\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "We will learn about what these functions do soon~\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "<b>Before we can even start making our model, we need to pre-process our data. We will be rescaling, applying filters, and resizing the images to be compatible for NN training</b>\n",
    "\n",
    "<h3>2. Train data </h3>\n",
    "\n",
    "#Define the directories where training/testing data located\n",
    ">\n",
    "    train_dir = 'data/train'\n",
    "    value_dir = 'data/test'\n",
    "\n",
    "#Divides image pixel values by 255 to scale down pixel values to normalized range between 0 and 1 for NN training\n",
    "\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    value_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "#Loads images from train_dir\n",
    ">  \n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "\n",
    "#Resize images to 48 x 48 pixels \n",
    ">\n",
    "        target_size = (48, 48),\n",
    "\n",
    "#Number of images processed in each batch \n",
    ">\n",
    "        batch_size = 64,\n",
    "\n",
    "#Convert images to grayscale (reduce dimensionality of input data from RGB to intensity)\n",
    ">\n",
    "        color_mode = \"grayscale\",\n",
    "\n",
    "#Labels for images are categorical values (Ex. Happy, Sad, Surprised, etc)\n",
    ">\n",
    "        class_mode = 'categorical'\n",
    "        )\n",
    "#Same process for test data    \n",
    ">    \n",
    "    value_generator = value_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size = (48, 48),\n",
    "        batch_size = 64,\n",
    "        color_mode = \"grayscale\",\n",
    "        class_mode = 'categorical'\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#5</mark> Create Model </h2>\n",
    "\n",
    "Continue in the train.py file. \n",
    "\n",
    "We can now start building our Convolutional Neural Network layer by layer using the sequential model.\n",
    "\n",
    "In order to create an <b>accurate</b> model, we are going to implement many convolutional layers to detect complex patterns, pooling layers to downsample the data, regularization techniques to prevent overfitting, flatten layers to prepare for fully connected layers, and dense layers to prepare for classification using activation functions.\n",
    "<hr>\n",
    "\n",
    ">\n",
    "    emotion_model = Sequential()\n",
    "\n",
    "#Adding 2 convolutional layers that are responsible for detecting local patterns in the input data\n",
    "#1st layer has 32 filters of size 3x3 pixels and applies the ReLU activation function, 2nd is the same except has 64 filters that allows the model to extract more complex patterns\n",
    "> \n",
    "    emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    emotion_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
    "\n",
    "#Pooling layers: Downsample data to look at small areas of the image (reduces spatial dimensions while retaining important features)\n",
    ">\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Regularization: Randomly sets input units to 0 to prevent overfitting (learn noise rather than actual signal)\n",
    ">\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "#More convolutional/pooling layers and regularization\n",
    ">\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "#Reshapes ouput from previous layers into 1D vector to prepare for the fully connected layers\n",
    ">\n",
    "    emotion_model.add(Flatten())\n",
    "\n",
    "#1st dense layer: 1024 neurons fully connected layer\n",
    ">\n",
    "    emotion_model.add(Dense(1024, activation='relu'))\n",
    "    emotion_model.add(Dropout(0.5))\n",
    "\n",
    "#2nd dense layer: 7 neurons which represents the # of possible output classes (emotions)\n",
    "#Uses softmax activation function to convert final layer's raw predicted values into a probability distribution over the different classes for classification\n",
    ">\n",
    "    emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#6</mark> Compile the Model </h2>\n",
    "\n",
    "\n",
    "#Prepare model for training by defining how it will measure loss, update its weights, and evaluate its prediction performance\n",
    ">\n",
    "    emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001, decay=1e-6), metrics=['accuracy'])\n",
    "\n",
    "#Train the model using a generator\n",
    ">\n",
    "    emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "\n",
    "        #Number of batches processed in each epoch\n",
    "        steps_per_epoch =28709 // 64,\n",
    "\n",
    "        #Number of times the entire dataset is passed through the model for training\n",
    "        epochs=50,\n",
    "\n",
    "        validation_data=validation_generator,\n",
    "\n",
    "        #Number of batches to be processed for validation in each epoch\n",
    "        validation_steps=7178 // 64 \n",
    "    )\n",
    "\n",
    "#Save learned parameters\n",
    ">\n",
    "    emotion_model.save_weights('model.h5')\n",
    "\n",
    "<hr>\n",
    "\n",
    "<b>Congratulations! All the code is in place, and you are now ready to compile and train your model!</b>\n",
    "\n",
    "- Go into your terminal and cd into the src folder\n",
    "- Run the line below and watch your neural network slowly but surely go through the epoch 50 times! \n",
    ">\n",
    "    python train.py\n",
    "\n",
    "- The output should look something like this. Watch the loss decrease and the accuracy increase !\n",
    "\n",
    "Start: \n",
    ">\n",
    "![](https://i.imgur.com/0WzdMVU.png)\n",
    "\n",
    "End: \n",
    ">\n",
    "![](https://i.imgur.com/SkgIxW5.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#7</mark> Fun! Create your personalized emojis :) </h2>\n",
    "\n",
    "<b>Now is time to design our emojis that will match with our realtime human facial detection</b>\n",
    "\n",
    "- Follow this [link](https://getavataaars.com/?accessoriesType=Blank&avatarStyle=Transparent&clotheColor=Red&clotheType=BlazerSweater&eyeType=Default&hairColor=Black&mouthType=Default&topType=LongHairNotTooLong) and create 7 seperate emojis for each of the 7 emotions\n",
    "- Save the images as angry.png, disgusted.png, fearful.png, happy.png, neutral.png, sad.png, surprised.png and place them in the \"emojis\" folder\n",
    "\n",
    "Here are some examples for inspo:\n",
    "\n",
    "Happy\n",
    "\n",
    "![](https://i.imgur.com/eLvInGM.png)\n",
    "\n",
    "Surprised\n",
    "\n",
    "![](https://i.imgur.com/Kxz21Ci.png)\n",
    "\n",
    "Disgusted\n",
    "\n",
    "![](https://i.imgur.com/nVGEWFB.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#8</mark> Implement GUI </h2>\n",
    "\n",
    "Now we will work in the <b>emoji.py</b>\n",
    "\n",
    "<h3>1. Import Packages</h3>\n",
    "\n",
    ">\n",
    "import tkinter as tk \n",
    "from tkinter import * \n",
    "import cv2 \n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "from cv2 import CAP_V4L2\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Dropout, Flatten \n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "import threading \n",
    "import time\n",
    "\n",
    "<h3>2. Copy Code </h3>\n",
    "\n",
    "Copy the following from the train.py file\n",
    ">\n",
    "\n",
    "    emotion_model = Sequential()\n",
    "\n",
    "    emotion_model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(48,48,1)))\n",
    "    emotion_model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\n",
    "\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Conv2D(128, kernel_size=(3,3), activation = 'relu'))\n",
    "    emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    emotion_model.add(Dropout(0.25))\n",
    "\n",
    "    emotion_model.add(Flatten())\n",
    "\n",
    "    emotion_model.add(Dense(1024, activation='relu'))\n",
    "    emotion_model.add(Dropout(0.5))\n",
    "\n",
    "    emotion_model.add(Dense(7, activation='softmax'))\n",
    "    emotion_model.load_weights('model.h5')\n",
    "\n",
    "\n",
    "<h3>3. Create dictionaries to later access emotion text and emojis</h3>\n",
    "\n",
    "> \n",
    "\n",
    "    #Disable use of OpenCV\n",
    "    cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "    #Create dictionary of emotions \n",
    "    emotion_dict = {\n",
    "        0: \"   Angry   \", \n",
    "        1: \"   Disgusted   \", \n",
    "        2: \"   Fearful   \", \n",
    "        3: \"   Happy   \", \n",
    "        4: \"   Neutral   \", \n",
    "        5: \"   Sad   \", \n",
    "        6: \"   Surprised   \"}\n",
    "\n",
    "    #Generate path\n",
    "    cur_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    #Navigate from current path into emojis folder and pick corresponding emotion\n",
    "    emoji_dist = {\n",
    "        0: cur_path+\"/emojis/angry.png\",\n",
    "        1: cur_path+\"/emojis/disgusted.png\",\n",
    "        2: cur_path+\"/emojis/fearful.png\",\n",
    "        3: cur_path+\"/emojis/happy.png\",\n",
    "        4: cur_path+\"/emojis/neutral.png\",\n",
    "        5: cur_path+\"/emojis/sad.png\",\n",
    "        6: cur_path+\"/emojis/surprised.png\",\n",
    "    }\n",
    "\n",
    "<h3> 4. Initialize variables and arrays </h3>\n",
    "\n",
    ">\n",
    "    #Stores the last captured video frame\n",
    "    global last_frame1\n",
    "    #Initializes array of 0s that will pass in image's RGB values \n",
    "    last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    global cap1 \n",
    "    #Initialize a list with a single element (index from emotion dictionary) \n",
    "    show_text = [0] \n",
    "    #Default emoji index (neutral)\n",
    "    show_text[0] = 4\n",
    "    #Event to synchronize subject and avatar threads\n",
    "    switch_thread_event = threading.Event()\n",
    "    #Event to signal the reads when to stop execution\n",
    "    stop_event = threading.Event()\n",
    "\n",
    "    # Debug counters\n",
    "    subject_count = 0 \n",
    "    avatar_count = 0\n",
    "\n",
    "<h3> 5. Create function to capture and read subject in frame</h3>\n",
    "\n",
    "> \n",
    "    \n",
    "#Function to capture video frames from webcam and detect emotions on the subject's face\n",
    "def show_subject():\n",
    "    global subject_count\n",
    "\n",
    "    #While program still running\n",
    "    while not stop_event.is_set():\n",
    "        # Wait for the switch thread event to be set\n",
    "        if not switch_thread_event.wait(5):\n",
    "            print(\"Subject Timeout occurred!\")\n",
    "            break\n",
    "\n",
    "        #Open webcam\n",
    "        cap1 = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap1.isOpened():\n",
    "            print(\"Can't find the camera\")\n",
    "        else:\n",
    "            print(\"Opened Camera\")\n",
    "\n",
    "        # frame 1 captures the video frame by frame, flag1 returns frame status \n",
    "        flag1, frame1 = cap1.read()\n",
    "\n",
    "        #Resize frame for faster processing\n",
    "        frame1 = cv2.resize(frame1, (600,500))\n",
    "\n",
    "        #Haarcascade classifier detects face in frame using pretrained info ab facial features\n",
    "        bounding_box = cv2.CascadeClassifier('C:\\Emojify\\data\\haarcascade_frontalface_default.xml')\n",
    "        #Converted to grayscale for better face detection accuracy\n",
    "        gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        #Note: Adjust scaleFactor and minNeighbors for prediction accuracy \n",
    "        #detectMultiScale function returns the coordinates and dimensions of the detected faces as rectangles\n",
    "        num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor = 1.1, minNeighbors=7)\n",
    "        \n",
    "        #For each detected face, a rectangle is drawn around it on the frame\n",
    "        for (x, y, w, h) in num_faces: \n",
    "            cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "            roi_gray_frame = gray_frame[y:y+h, x:x+w]\n",
    "            #Region of interest resized to the expected input size for the emotion recognition model\n",
    "            #Face image converted into a numpy array\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "            #Face image is passed through the pre-trained emotion recognition model \n",
    "            #Predict function returns a probability distribution over different emotion classes\n",
    "            prediction = emotion_model.predict(cropped_img)\n",
    "            #Index of the highest probability in the prediction array is calculated to determine the predicted emotion class\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            #Retrieve emotion label & display (subject)\n",
    "            #Corresponding emotion label is retrieved from the emotion_dict dictionary & displayed in window\n",
    "            cv2.putText(frame1, emotion_dict[maxindex], (x+40, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            show_text[0]=maxindex\n",
    "\n",
    "            #For debugging\n",
    "            current_time_ms = time.time_ns() // 10**6\n",
    "            subject_count = subject_count + 1 \n",
    "            print(\"Current time for subject\", current_time_ms, show_text[0], subject_count)\n",
    "\n",
    "        print(\"flag1\", flag1)\n",
    "\n",
    "        #If frame is not returned\n",
    "        if flag1 is None:\n",
    "            print(\"Major error! Frame is not returned!\")\n",
    "        #If frame captured successfully \n",
    "        elif flag1 == True:\n",
    "            global last_frame1\n",
    "            last_frame1 = frame1.copy()\n",
    "            pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(pic)\n",
    "            #Represents image element in GUI\n",
    "            imgtk = ImageTk.PhotoImage(image=img)\n",
    "            #Updates displayed image\n",
    "            lmain.imgtk = imgtk\n",
    "            lmain.configure(image=imgtk)\n",
    "\n",
    "        # After loop release webcam to be used in this program\n",
    "        #cap1.release()\n",
    "        # Destroy all the windows\n",
    "        cv2.destroyAllWindows()\n",
    "        #Never prints bc webcam not released \n",
    "        print('webcam destroyed')\n",
    "\n",
    "        #Once process frame, pause for a bit\n",
    "        time.sleep(0.3)\n",
    "        #Update window\n",
    "        root.update()\n",
    "\n",
    "        # Reset the switch_thread_event\n",
    "        switch_thread_event.clear()\n",
    "        switch_thread_event.set()\n",
    "\n",
    "    print(\"Subject thread is finished\")\n",
    "\n",
    "<h3> 6. Create avatar function to sink subject emotions with displayed emoji</h3>\n",
    "\n",
    ">\n",
    "\n",
    "    def show_avatar():\n",
    "    global avatar_count\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        # Wait for the switch_thread_event to be set\n",
    "        if not switch_thread_event.wait(5):\n",
    "            print(\"Avatar Timeout occurred!\")\n",
    "            break\n",
    "\n",
    "        #More debugging\n",
    "        emoji_index = show_text[0]\n",
    "        avatar_count = avatar_count + 1 \n",
    "        current_time_ms = time.time_ns() // 10**6\n",
    "        print(\"Current time for avatar\", current_time_ms, emoji_index, avatar_count)\n",
    "\n",
    "        frame2 = cv2.imread(emoji_dist[emoji_index])\n",
    "        pic2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)\n",
    "        img2 = Image.fromarray(pic2)\n",
    "        imgtk2 = ImageTk.PhotoImage(image=img2)\n",
    "        lmain2.imgtk2 = imgtk2\n",
    "        lmain3.configure(text = emotion_dict[emoji_index], font = ('arial', 45, 'bold'))\n",
    "        lmain2.configure(image=imgtk2)\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "        root.update()\n",
    "\n",
    "        # Reset the switch_thread_event\n",
    "        switch_thread_event.clear()\n",
    "        switch_thread_event.set()\n",
    "    \n",
    "    print (\"Avatar thread is finished\")\n",
    "\n",
    "<h3> 7. Wrapper functions to call stop and switch tread functions</h3>\n",
    "\n",
    ">\n",
    "        \n",
    "    def stop_threads():\n",
    "        global stop_event\n",
    "        global switch_thread_event\n",
    "        #Stop events, clear threads\n",
    "        stop_event.set()\n",
    "        switch_thread_event.clear()\n",
    "\n",
    "    def wrapper_quit():\n",
    "        stop_threads()\n",
    "        print(\"After stop camera thread \", stop_event.is_set(), switch_thread_event.is_set())\n",
    "        #Close GUI window\n",
    "        root.destroy()\n",
    "\n",
    "<h3> 8. if main & using tkInter to display frontend by placing and packing labels</h3>\n",
    "\n",
    ">\n",
    "    \n",
    "#Only be executed if script run directly \n",
    "if __name__ == '__main__':\n",
    "    frame_number = 0\n",
    "    root = tk.Tk()\n",
    "    \n",
    "    #Create labels to contain images/video\n",
    "    #Human video\n",
    "    lmain = tk.Label(master = root, padx = 50, bd = 10)\n",
    "    #Emoji\n",
    "    lmain2 = tk.Label(master = root, bd = 25)\n",
    "    #Quit button for entire program\n",
    "    lmain3 = tk.Label(master=root, bd = 20, fg = \"#CDCDCD\", bg = 'blue', font=(\"Arial\", 30))\n",
    "    \n",
    "    #Packing and placing in location \n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x = 30, y = 100)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x = 1000, y = 600)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x = 700, y = 100)\n",
    "\n",
    "    root.title(\"Translating Realtime Human Facial Expressions to an Emoji using a Trained CNN\")\n",
    "    root.geometry(\"1400x900+100+10\")\n",
    "    root['bg'] = 'black'\n",
    "    switch_thread_event.set() \n",
    "    subject_thread = threading.Thread(target = show_subject)\n",
    "    avatar_thread = threading.Thread(target = show_avatar)\n",
    "    #When button pressed, function specified by command parameter will be executed\n",
    "    exitButton = Button(root, text = 'Quit', fg = \"red\", command = wrapper_quit, font = ('arial', 30, 'bold')).pack(side = TOP)\n",
    "\n",
    "    subject_thread.start()\n",
    "    avatar_thread.start()\n",
    "\n",
    "    print(\"Before main loop\")\n",
    "    root.mainloop()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><mark>#9</mark> Testing & Improving Accuracy</h2>\n",
    "\n",
    "<b> Do a happy dance! You have now officially coded the base to start testing your final product!</b>\n",
    "\n",
    "- Run python3 emoji.py in terminal and watch magic happen!\n",
    "\n",
    "Some advise about <mark>improving accuracy</mark>\n",
    "- Subject: Wear plain clothes (avoid graphics), Hair out of face, One person in frame at a time, ONLY use face to change expression (face detection XML file doesn't take into account hands or any other body parts)\n",
    "- Adjust scalefactor and minNeighbors in emoji.py \n",
    "- Consider adding more diverse photos in data set and retraining the model \n",
    "\n",
    "NOTES about the model\n",
    "1. Accuracy of certain facial expressions \n",
    "- Happy (Curve of mouth, Eyes are smaller) - GOOD \n",
    "- Surprised (Form an O with mouth, big eyes) - GOOD \n",
    "- Neutral (Flat line of mouth, eyebrows even, eyes not big) - OKAY \n",
    "- Sad (Mouth curved down, smaller eyes) - OKAY \n",
    "- Anger (exaggerated frustrated eyebrows and mouth curved downword) - OKAY\n",
    "- Disgusted - BAD RARELY DETECTS \n",
    "- Fearful (Wide eyes, mouth open) - BAD, EASILY CONFUSED WITH SURPRISED\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra notes\n",
    "\n",
    "- Dense layer: Each neuron in the layer is connected to EVERY neuron in the previous layer\n",
    "- Drop out Layer: Randomly deactivates neurons to prevent a model that becomes too specialized and preforms extremely well on the training data but fails to generalize and make accurate predictions on new, unseen data \n",
    "- Flatten layer: Takes complex, structured data (images) and makes its impler by converting it into a flat, 1D array. Useful when transitioning from convolutionl/pooling layers to subsequent layers to process the data as a simpler, linear sequence. Easier for NN to learn patterns and make predictions \n",
    "- Keras: Sequential vs Functional\n",
    "    - Sequential (Create model layer by layer)\n",
    "    - Functional (A layer can connect to any layer, much more complex)\n",
    "- CNN: Simple pplication of filter to an input that results in an activation. Certain inputs and thresholds. When Input meets those thresholds there is an activation \n",
    "- Certain type of input --> repeats itself --> feature map forms\n",
    "- Activation function: mathematical func that determines whether the neuron should be activated based on the input it recieves. Introduces non-linearity to the network, allowing it to learn and model complex relationships between input data and output predictions \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
